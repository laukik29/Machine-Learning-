{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "For this assigment you will be implementing an algorithm which is very commonly used in practice: `Random Forest`.  \n",
    "So what is `Random Forest`? To answer this question, let's first learn a couple of new statistical concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "Recall `Central Limit Theorem` from your probability and statistics course. It states that if you have a population with mean $\\mu$ and standard deviation $\\sigma$ and take sufficiently large random samples from the population with replacement, then the distribution of the sample means will be approximately normally distributed. This will hold true regardless of whether the source population is normal or skewed, provided the sample size is sufficiently large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretly, we can say that if:\n",
    "\n",
    "$$ X_1, X_2, \\ldots, X_n \\sim P(X) $$\n",
    "\n",
    "where $X_i$ are i.i.d. random samples from any distribution $P(X)$ with mean $\\mu$ and standard deviation $\\sigma$, then we have:\n",
    "$$ \\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i \\sim N(\\mu, \\frac{\\sigma}{\\sqrt{n}}) $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ Var(\\bar{X}) = \\frac{\\sigma^2}{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the case of Machine Learning, we are many times interested in decreasing the variance of our model.  \n",
    "- In `Ensemble Methods`, we do so by training multiple different machine learning models such as (Logistic Regression, Decision Trees, Support Vector Machines, etc.) and then combining them in some way to get a final prediction.  \n",
    "- Doing so, we are able to decrease the variance of our model and thus improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula for variance reduction works only if the samples are independent. In practice, this is not the case. There tends to be a certain amount of correlation between the samples. The equation for variance reduction can be modified to account for this correlation.\n",
    "\n",
    "$$ Var(\\bar{X}) = \\rho \\sigma^2 + \\frac{1 - \\rho}{n} \\sigma^2$$\n",
    "\n",
    "where $\\rho$ is the correlation between the samples.\n",
    "\n",
    "Notice that if $\\rho = 0$, then we get the original formula for variance reduction. and if $\\rho = 1$, then there is no variance reduction at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many different forms of ensemble methods. Each of those try to decrease this correlation term ($\\rho$) as much as possible.  \n",
    "In this assignment(`Random Forest`), we will be focusing on one specific type called `Bagging`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "For the baseline model, implement a normal `Decision Tree` classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Proprocessing (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_val = pd.read_csv('val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Party</th>\n",
       "      <th>Feature-1</th>\n",
       "      <th>Feature-2</th>\n",
       "      <th>Feature-3</th>\n",
       "      <th>Feature-4</th>\n",
       "      <th>Feature-5</th>\n",
       "      <th>Feature-6</th>\n",
       "      <th>Feature-7</th>\n",
       "      <th>Feature-8</th>\n",
       "      <th>Feature-9</th>\n",
       "      <th>Feature-10</th>\n",
       "      <th>Feature-11</th>\n",
       "      <th>Feature-12</th>\n",
       "      <th>Feature-13</th>\n",
       "      <th>Feature-14</th>\n",
       "      <th>Feature-15</th>\n",
       "      <th>Feature-16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>democrat</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>democrat</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democrat</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>democrat</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>democrat</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Party Feature-1 Feature-2 Feature-3 Feature-4 Feature-5 Feature-6  \\\n",
       "0  democrat         n         y         y         n         y         y   \n",
       "1  democrat         y         n         y         n         n         n   \n",
       "2  democrat         n         y         y         n         n         y   \n",
       "3  democrat         y         y         y         n         n         n   \n",
       "4  democrat         n         n         y         n         n         n   \n",
       "\n",
       "  Feature-7 Feature-8 Feature-9 Feature-10 Feature-11 Feature-12 Feature-13  \\\n",
       "0         y         n         y          y          y          n          y   \n",
       "1         y         y         y          n          n          n          y   \n",
       "2         n         y         y          y          y          n          y   \n",
       "3         y         y         y          n          y          n          n   \n",
       "4         y         y         y          n          n          n          n   \n",
       "\n",
       "  Feature-14 Feature-15 Feature-16  \n",
       "0          y          n          y  \n",
       "1          n          y          y  \n",
       "2          n          y          y  \n",
       "3          n          y          y  \n",
       "4          n          y          y  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Party</th>\n",
       "      <th>Feature-1</th>\n",
       "      <th>Feature-2</th>\n",
       "      <th>Feature-3</th>\n",
       "      <th>Feature-4</th>\n",
       "      <th>Feature-5</th>\n",
       "      <th>Feature-6</th>\n",
       "      <th>Feature-7</th>\n",
       "      <th>Feature-8</th>\n",
       "      <th>Feature-9</th>\n",
       "      <th>Feature-10</th>\n",
       "      <th>Feature-11</th>\n",
       "      <th>Feature-12</th>\n",
       "      <th>Feature-13</th>\n",
       "      <th>Feature-14</th>\n",
       "      <th>Feature-15</th>\n",
       "      <th>Feature-16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Party  Feature-1  Feature-2  Feature-3  Feature-4  Feature-5  Feature-6  \\\n",
       "0      0          0          1          1          0          1          1   \n",
       "1      0          1          0          1          0          0          0   \n",
       "2      0          0          1          1          0          0          1   \n",
       "3      0          1          1          1          0          0          0   \n",
       "4      0          0          0          1          0          0          0   \n",
       "\n",
       "   Feature-7  Feature-8  Feature-9  Feature-10  Feature-11  Feature-12  \\\n",
       "0          1          0          1           1           1           0   \n",
       "1          1          1          1           0           0           0   \n",
       "2          0          1          1           1           1           0   \n",
       "3          1          1          1           0           1           0   \n",
       "4          1          1          1           0           0           0   \n",
       "\n",
       "   Feature-13  Feature-14  Feature-15  Feature-16  \n",
       "0           1           1           0           1  \n",
       "1           1           0           1           1  \n",
       "2           1           0           1           1  \n",
       "3           0           0           1           1  \n",
       "4           0           0           1           1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the categorical columns\n",
    "d = {'y': 1, 'n': 0}\n",
    "d_party = {'democrat': 0, 'republican': 1}\n",
    "for col in df_train.columns:\n",
    "    if col != 'Party':\n",
    "        df_train[col] = df_train[col].map(d)\n",
    "        df_val[col] = df_val[col].map(d)\n",
    "    else:\n",
    "        df_train[col] = df_train[col].map(d_party)\n",
    "        df_val[col] = df_val[col].map(d_party)\n",
    "        \n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Decision Tree class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, dataset: pd.DataFrame, features: list, target: str):\n",
    "        \"\"\"Initializes the Node class. This class represents the current state of the dataset after a split has been made.\n",
    "\n",
    "        dataset : Current state of the dataset\n",
    "        features : A list of features to consider for the split\n",
    "        target : The target variable\n",
    "\n",
    "        best_feature: Feature which gives the highest information gain. This starts off as None and is updated after the split\n",
    "        best_feature_values: A dictionary which contains the unique values of the best feature and the corresponding subset of the dataset\n",
    "\n",
    "        class_names: A list of the unique values in the target variable\n",
    "        target_probabilities: A NumPy array which contains the probabilities of each unique value in the target attribute\n",
    "        entropy: Entropy of the target variable in the given dataset\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): Current state of the dataset\n",
    "            features (list): A list of features to consider for the split\n",
    "            target (str): The target variable\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "\n",
    "        self.best_feature = None\n",
    "        self.best_feature_values = {}\n",
    "\n",
    "        self.class_names = dataset[target].unique().tolist()\n",
    "        self.target_probabilities = self.calculate_target_probabilities(self.dataset)\n",
    "        self.entropy = self.calculate_entropy(self.dataset)\n",
    "    \n",
    "    def calculate_target_probabilities(self, dataset: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"This function calculates the probabilities of each unique value in the target variable.\n",
    "        You can think of this as the prior probabilities (as in Naive Bayes)\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): The dataset over which we want to calculate the probabilities\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Returns a NumPy array which contains the probabilities of each unique value in the target attribute\n",
    "        \"\"\"\n",
    "        target_probabilties = dataset[self.target].value_counts(normalize=True).values\n",
    "        return target_probabilties\n",
    "    \n",
    "    def calculate_entropy(self, dataset: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculates the entropy of the target variable in the given dataset\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): dataset over which we want to calculate the entropy\n",
    "\n",
    "        Returns:\n",
    "            float: Entropy of the target variable in the given dataset\n",
    "        \"\"\"\n",
    "        target_probabilities = dataset[self.target].value_counts(normalize=True).values\n",
    "        target_log_probabilities = np.log2(target_probabilities)\n",
    "\n",
    "        entropy = -np.sum(target_probabilities * target_log_probabilities)\n",
    "        return entropy\n",
    "\n",
    "    def calculate_information_gain(self, feature: str) -> float:\n",
    "        \"\"\"Calculates the information gain for the gievn feature\n",
    "\n",
    "        Args:\n",
    "            feature (str): Feature for which we want to calculate the information gain\n",
    "\n",
    "        Returns:\n",
    "            float: Information gain for the given feature\n",
    "        \"\"\"\n",
    "\n",
    "        # Find the unique values of the feature\n",
    "        feature_values = self.dataset[feature].unique()\n",
    "\n",
    "        # Find the probabilities of each unique value\n",
    "        feature_probabilities = self.dataset[feature].value_counts(normalize=True).values\n",
    "\n",
    "        # Find the entropy of each unique value\n",
    "        feature_entropies = []\n",
    "        for feature_value in feature_values:\n",
    "            feature_subset = self.dataset[self.dataset[feature] == feature_value]\n",
    "            feature_subset_entropy = self.calculate_entropy(feature_subset)\n",
    "            feature_entropies.append(feature_subset_entropy)\n",
    "\n",
    "        # Find the weighted average of the entropies\n",
    "        feature_entropies = np.array(feature_entropies)\n",
    "        weighted_average = np.sum(feature_probabilities * feature_entropies)\n",
    "\n",
    "        # Find the information gain\n",
    "        information_gain = self.entropy - weighted_average\n",
    "        return information_gain\n",
    "\n",
    "    def find_best_feature(self) -> str:\n",
    "        \"\"\"Finds the best feature which gives the highest information gain. This is the feature which we will use to split the dataset\n",
    "\n",
    "        Returns:\n",
    "            str: Feature which gives the highest information gain\n",
    "        \"\"\"\n",
    "        # Get information gain for all features and select the one with the highest information gain\n",
    "        information_gains = []\n",
    "        for feature in self.features:\n",
    "            information_gain = self.calculate_information_gain(feature)\n",
    "            information_gains.append(information_gain)\n",
    "\n",
    "        information_gains = np.array(information_gains)\n",
    "        best_feature_index = np.argmax(information_gains)\n",
    "        best_feature = self.features[best_feature_index]\n",
    "        return best_feature\n",
    "    \n",
    "    def is_pure(self) -> bool:\n",
    "        \"\"\"Checks if the node is pure. A node is pure if all the target values are the same\n",
    "\n",
    "        Returns:\n",
    "            bool: Pure or not\n",
    "        \"\"\"\n",
    "        return self.entropy == 0\n",
    "\n",
    "    def predict(self) -> str:\n",
    "        \"\"\"Predicts the class of the current node. This is the class which has the highest probability\n",
    "\n",
    "        Returns:\n",
    "            str: Prediction\n",
    "        \"\"\"\n",
    "        idx = np.argmax(self.target_probabilities)\n",
    "        prediction = self.class_names[idx]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, dataset: pd.DataFrame, features: list, target: str, node: Node=None, depth: int=1):\n",
    "        \"\"\"Trains the decision tree model on the given dataset\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): Dataset on which we want to train the model\n",
    "            features (list): List of features to consider for training\n",
    "            target (str): The target attribute\n",
    "            node (Node): The current node. Defaults to None.\n",
    "            depth (int): Current depth of the tree.\n",
    "        \"\"\"\n",
    "\n",
    "        if node is not None and node.is_pure(): # Do not split if the node is pure\n",
    "            return\n",
    "        \n",
    "        if self.max_depth is not None and depth > self.max_depth: # Do not split if the max depth is reached\n",
    "            return\n",
    "        \n",
    "        if not features: # Do not split if there are no features left\n",
    "            return\n",
    "        \n",
    "        if node is None:\n",
    "            node = Node(dataset, features, target)\n",
    "            self.root = node\n",
    "        \n",
    "        # Find the best feature to split on\n",
    "        best_feature = node.find_best_feature()\n",
    "        node.best_feature = best_feature # Set the best feature of the node\n",
    "        best_feature_values = dataset[best_feature].unique() # Find the unique values of the best feature\n",
    "\n",
    "        for best_feature_value in best_feature_values:\n",
    "\n",
    "            # Create a subset of the dataset which contains only the current best feature value and remove the best feature from the dataset\n",
    "            #! NOTE: Remember to create a copy of the dataset while removing the feature. Otherwise, the original dataset will be modified\n",
    "            best_feature_subset = dataset[dataset[best_feature] == best_feature_value]\n",
    "            best_feature_subset = best_feature_subset.drop(columns=[best_feature])\n",
    "\n",
    "            # Create a subset of the features which does not contain the best feature\n",
    "            best_feature_subset_features = list(best_feature_subset.columns)\n",
    "            best_feature_subset_features.remove(target)\n",
    "\n",
    "            # Create a new node for the best split\n",
    "            best_feature_subset_root = Node(best_feature_subset, best_feature_subset_features, target)\n",
    "            node.best_feature_values[best_feature_value] = best_feature_subset_root\n",
    "\n",
    "\n",
    "            # Recursively fit the model on the best feature subset\n",
    "            self.fit(best_feature_subset, best_feature_subset_features, target, node=best_feature_subset_root, depth=depth+1)\n",
    "        \n",
    "    def predict(self, features: pd.Series) -> tuple:\n",
    "        \"\"\"Predict the class for the given features using the trained model\n",
    "\n",
    "        Args:\n",
    "            features (pd.Series): features\n",
    "\n",
    "        Returns:\n",
    "            str: Prediction of the class\n",
    "            list: List of decisions made by the model. These are the features on which the model split the dataset\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        node = self.root\n",
    "        decisions = []\n",
    "\n",
    "        while node.best_feature is not None:\n",
    "            decisions.append(node.best_feature)\n",
    "            feature_value = features[node.best_feature]\n",
    "            if feature_value not in node.best_feature_values:\n",
    "                break\n",
    "            node = node.best_feature_values[feature_value]\n",
    "        \n",
    "        prediction = node.predict()\n",
    "        return prediction, decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Party', 'Feature-1', 'Feature-2', 'Feature-3', 'Feature-4',\n",
      "       'Feature-5', 'Feature-6', 'Feature-7', 'Feature-8', 'Feature-9',\n",
      "       'Feature-10', 'Feature-11', 'Feature-12', 'Feature-13', 'Feature-14',\n",
      "       'Feature-15', 'Feature-16'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_train.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model on the training data and report the accuracy and [F1 score](https://en.wikipedia.org/wiki/F-score) on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_train.columns.tolist()\n",
    "features.remove('Party')\n",
    "target = 'Party'\n",
    "dtree = DecisionTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTree(max_depth=5)\n",
    "clf.fit(df_train, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Party</th>\n",
       "      <th>Feature-1</th>\n",
       "      <th>Feature-2</th>\n",
       "      <th>Feature-3</th>\n",
       "      <th>Feature-4</th>\n",
       "      <th>Feature-5</th>\n",
       "      <th>Feature-6</th>\n",
       "      <th>Feature-7</th>\n",
       "      <th>Feature-8</th>\n",
       "      <th>Feature-9</th>\n",
       "      <th>Feature-10</th>\n",
       "      <th>Feature-11</th>\n",
       "      <th>Feature-12</th>\n",
       "      <th>Feature-13</th>\n",
       "      <th>Feature-14</th>\n",
       "      <th>Feature-15</th>\n",
       "      <th>Feature-16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>republican</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>republican</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democrat</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>republican</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>democrat</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Party Feature-1 Feature-2 Feature-3 Feature-4 Feature-5 Feature-6  \\\n",
       "0  republican         n         n         n         y         y         y   \n",
       "1  republican         y         y         y         y         n         n   \n",
       "2    democrat         y         n         y         n         n         n   \n",
       "3  republican         n         y         y         y         y         y   \n",
       "4    democrat         y         y         y         n         n         n   \n",
       "\n",
       "  Feature-7 Feature-8 Feature-9 Feature-10 Feature-11 Feature-12 Feature-13  \\\n",
       "0         y         n         n          y          n          y          n   \n",
       "1         y         y         y          y          y          n          n   \n",
       "2         y         y         y          n          n          n          n   \n",
       "3         n         n         n          y          n          y          y   \n",
       "4         y         y         y          n          y          n          n   \n",
       "\n",
       "  Feature-14 Feature-15 Feature-16  \n",
       "0          y          y          y  \n",
       "1          y          n          y  \n",
       "2          n          y          y  \n",
       "3          y          n          y  \n",
       "4          n          y          y  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('Val.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(actual, predicted):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == 1 and predicted[i] == 1:\n",
    "            true_positives += 1\n",
    "        elif actual[i] == 0 and predicted[i] == 1:\n",
    "            false_positives += 1\n",
    "        elif actual[i] == 1 and predicted[i] == 0:\n",
    "            false_negatives += 1\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "\n",
    "    print(f'True positives: {true_positives}')\n",
    "    print(f'False positives: {false_positives}')\n",
    "    print(f'False negatives: {false_negatives}')\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'best_feature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32me:\\F\\Github\\Machine Learning\\Machine-Learning-\\ML BITS F464\\Kaggle\\lab5-20230927\\eval.ipynb Cell 22\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m y_pred \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m idx, row \u001b[39min\u001b[39;00m df_val\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     y_pred\u001b[39m.\u001b[39mappend(dtree\u001b[39m.\u001b[39;49mpredict(row)[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# calculate F1 score manually \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m score \u001b[39m=\u001b[39m score(y_actual, y_pred)\n",
      "\u001b[1;32me:\\F\\Github\\Machine Learning\\Machine-Learning-\\ML BITS F464\\Kaggle\\lab5-20230927\\eval.ipynb Cell 22\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m decisions \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mwhile\u001b[39;00m node\u001b[39m.\u001b[39;49mbest_feature \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     decisions\u001b[39m.\u001b[39mappend(node\u001b[39m.\u001b[39mbest_feature)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X30sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     feature_value \u001b[39m=\u001b[39m features[node\u001b[39m.\u001b[39mbest_feature]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'best_feature'"
     ]
    }
   ],
   "source": [
    "# predict on validation data\n",
    "# remove party column from validation data\n",
    "y_actual = df_val['Party'].tolist()\n",
    "df_val = df_val.drop(columns=['Party'])\n",
    "\n",
    "y_pred = []\n",
    "for idx, row in df_val.iterrows():\n",
    "    y_pred.append(dtree.predict(row)[0])\n",
    "\n",
    "# calculate F1 score manually \n",
    "score = score(y_actual, y_pred)\n",
    "print(f\"the F1 score is {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging ([Bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) Aggregation) is a method of constructing an ensemble of classifiers by training each classifier on a random subset (bootstrap sample) of the training set. Concretly,\n",
    "\n",
    "- Given a training set $X = x_1, x_2, \\ldots, x_n$ with labels $Y = y_1, y_2, \\ldots, y_n$.\n",
    "- We create $k$ bootstrap samples $X_1, X_2, \\ldots, X_k$ by sampling $r$ examples from $X$ uniformly at random with replacement.\n",
    "- We train $k$ different classifiers $h_1, h_2, \\ldots, h_k$ on the bootstrap samples $X_1, X_2, \\ldots, X_k$.\n",
    "- We combine the predictions of each classifier using majority voting.\n",
    "\n",
    "In doing so, we are able to decrease the correlation between the classifiers and thus decrease the variance of our model.  \n",
    "Notice that if we use the same training set to train each classifier, then the correlation between the classifiers will be very high ($\\rho \\approx 1$) and thus there will be almost no variance reduction at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function which creates bootstrap samples from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(df: pd.DataFrame, n_samples: int, sample_fraction: float, target: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"Generate bootstrap samples using the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to generate bootstrap samples from.\n",
    "        n_samples (int): The number of bootstrap samples to generate.\n",
    "        sample_fraction (float): The fraction of the dataframe to use for each sample with replacement. (between 0 and 1)\n",
    "        target (str): The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of bootstrap samples.\n",
    "    \"\"\"\n",
    "\n",
    "    bootstrap_samples = []\n",
    "    for i in range(n_samples):\n",
    "        bootstrap_sample = df.sample(frac=sample_fraction, replace=True)\n",
    "        bootstrap_samples.append(bootstrap_sample)\n",
    "\n",
    "    \n",
    "    return bootstrap_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take 'n' bootstrap samples from the training data and train a `Decision Tree` classifier on each of them. Make sure you use the `DecisionTree` class you implemented above to train the models and store the trained trees in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 10\n",
    "sample_fraction = 0.8\n",
    "target = \"Party\" # TODO: Set the target column\n",
    "trees = []\n",
    "\n",
    "bootstrap_samples = get_bootstrap_samples(df_train, n_trees, sample_fraction, target)\n",
    "\n",
    "# Write your code below\n",
    "\n",
    "# Create a decision tree for each bootstrap sample and append it to the trees list\n",
    "for _s in bootstrap_samples:\n",
    "    tree = DecisionTree()\n",
    "    tree.fit(_s, features, target)\n",
    "    trees.append(tree)\n",
    "\n",
    "# Write code to predict the class of the current set of features and append the prediction to the preds list\n",
    "# Also append the decisions made by the model to the decisions list\n",
    "preds = []\n",
    "decisions = []\n",
    "for _, row in df_val.iterrows():\n",
    "    decision_importance = []\n",
    "    for tree in trees:\n",
    "        pred, decision = tree.predict(row)\n",
    "        decision_importance.append(pred)\n",
    "        decisions.append(decision)\n",
    "    \n",
    "    pred = max(set(decision_importance), key=decision_importance.count)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained trees to make predictions on the validation data and report the accuracy and [F1 Score](https://en.wikipedia.org/wiki/F-score) on the validation data. To make the prediction follow the following steps:\n",
    "- For each data point in the validation set, make predictions using each of the trained trees. (you will have 'n_trees' predictions for each data point)\n",
    "- Combine the predictions of each classifier using majority voting. \n",
    "- eg. If you have 5 trees and 3 of them predict __class 1__ and 2 of them predict __class 2__, then the final prediction will be __class 1__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\F\\Github\\Machine Learning\\Machine-Learning-\\ML BITS F464\\Kaggle\\lab5-20230927\\eval.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     aggregate_preds\u001b[39m.\u001b[39mappend(\u001b[39mmax\u001b[39m(\u001b[39mset\u001b[39m(votes), key\u001b[39m=\u001b[39mvotes\u001b[39m.\u001b[39mcount))\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# calculate F1 score manually\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m score \u001b[39m=\u001b[39m score(y_actual, aggregate_preds)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/F/Github/Machine%20Learning/Machine-Learning-/ML%20BITS%20F464/Kaggle/lab5-20230927/eval.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthe F1 score is \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "# predict on validation data\n",
    "\n",
    "aggregate_decisions = []\n",
    "\n",
    "for tree in trees:\n",
    "    y_pred = []\n",
    "    for idx, row in df_val.iterrows():\n",
    "        y_pred.append(tree.predict(row)[0])\n",
    "    aggregate_decisions.append(y_pred)\n",
    "\n",
    "\n",
    "# combine the predictions from all the trees\n",
    "aggregate_preds = []\n",
    "for i in range(len(aggregate_decisions[0])):\n",
    "    votes = []\n",
    "    for j in range(len(aggregate_decisions)):\n",
    "        votes.append(aggregate_decisions[j][i])\n",
    "    aggregate_preds.append(max(set(votes), key=votes.count))\n",
    "\n",
    "# calculate F1 score manually\n",
    "score = score(y_actual, aggregate_preds)\n",
    "print(f\"the F1 score is {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Random Forest` is a special type of bagging algorithm where we not only train each classifier on a bootstrap sample of the training data but also a random subset of the features. This is done to further decrease the correlation between the classifiers. Concretly,\n",
    "\n",
    "- Given a training set $X = x_1, x_2, \\ldots, x_n$ with labels $Y = y_1, y_2, \\ldots, y_n$.\n",
    "- We create $k$ bootstrap samples $X_1, X_2, \\ldots, X_k$ by sampling $r$ examples from $X$ uniformly at random with replacement.\n",
    "- Each time we create a bootstrap sample, we also select a random subset of the features $F_i$ to train the classifier on.\n",
    "- We train $k$ different classifiers $h_1, h_2, \\ldots, h_k$ on the bootstrap samples $X_1, X_2, \\ldots, X_k$.\n",
    "- We combine the predictions of each classifier using majority voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement this new version of bootstrap sampling below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_forest_bootstrap_samples(df: pd.DataFrame, n_samples: int, sample_fraction: float, feature_fraction: float, target: str) -> Tuple[List[pd.DataFrame], List[str]]:\n",
    "    \"\"\"Generate bootstrap samples using the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to generate bootstrap samples from.\n",
    "        n_samples (int): The number of bootstrap samples to generate.\n",
    "        sample_fraction (float): The fraction of the dataframe to use for each sample with replacement. (between 0 and 1)\n",
    "        feature_fraction (float): The fraction of features to use for each sample. (between 0 and 1)\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of bootstrap samples.\n",
    "        List[str]: A list of features used for each sample.\n",
    "    \"\"\"\n",
    "\n",
    "    samples = []\n",
    "    features = []\n",
    "\n",
    "    target_features = df.columns.tolist()\n",
    "    target_features.remove(target)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        bootstrap_sample = df.sample(frac=sample_fraction, replace=True)\n",
    "\n",
    "        # get random features\n",
    "        n_features = int(feature_fraction * len(df.columns))\n",
    "        feats = np.random.choice(target_features, n_features, replace=False)\n",
    "        feats = list(feats)\n",
    "        feats.append(target)\n",
    "        features.append(feats)\n",
    "\n",
    "        # keep only the selected features\n",
    "        bootstrap_sample = bootstrap_sample[feats]\n",
    "        samples.append(bootstrap_sample)\n",
    "    \n",
    "    return samples, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take 'n' bootstrap samples from the training data and train a `Decision Tree` classifier on each of them. Make sure you use the `DecisionTree` class you implemented above to train the models and store the trained trees in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Feature-11  Feature-12  Feature-5  Feature-3  Feature-9  Feature-15  \\\n",
      "226           1           1          0          1          1           1   \n",
      "8             0           0          0          1          1           1   \n",
      "248           0           0          1          0          0           0   \n",
      "8             0           0          0          1          1           1   \n",
      "193           1           0          0          1          1           1   \n",
      "..          ...         ...        ...        ...        ...         ...   \n",
      "303           1           1          1          0          0           0   \n",
      "149           0           1          1          1          1           1   \n",
      "246           1           1          1          0          0           0   \n",
      "83            0           1          1          1          0           0   \n",
      "111           1           0          0          1          1           1   \n",
      "\n",
      "     Feature-10  Feature-4  Feature-6  Feature-13  Feature-8  Party  \n",
      "226           0          0          1           0          1      0  \n",
      "8             0          0          0           0          1      0  \n",
      "248           0          1          1           1          0      1  \n",
      "8             0          0          0           0          1      0  \n",
      "193           0          0          0           0          1      0  \n",
      "..          ...        ...        ...         ...        ...    ...  \n",
      "303           1          1          1           1          0      1  \n",
      "149           1          0          0           0          1      0  \n",
      "246           0          0          1           0          0      0  \n",
      "83            1          1          1           1          0      1  \n",
      "111           1          0          1           0          1      0  \n",
      "\n",
      "[295 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "n_trees = 10\n",
    "sample_fraction = 0.8\n",
    "feature_fraction = 0.7\n",
    "target = \"Party\" # TODO: Set the target column\n",
    "trees = []\n",
    "\n",
    "bootstrap_samples, sample_features = get_random_forest_bootstrap_samples(df_train, n_trees, sample_fraction, feature_fraction, target)\n",
    "\n",
    "# Write your code below\n",
    "print(bootstrap_samples[0])\n",
    "\n",
    "# # Create a decision tree for each bootstrap sample and append it to the trees list\n",
    "# for i in range(len(bootstrap_samples)):\n",
    "#     tree = DecisionTree()\n",
    "#     tree.fit(bootstrap_samples[i], sample_features[i], target)\n",
    "#     trees.append(tree)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained trees to make predictions on the validation data and report the accuracy and [F1 Score](https://en.wikipedia.org/wiki/F-score) on the validation data. Use the sample prediction method as in bagging. Try modifying the different parameters to get the best results on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to try out once you have a working implementation and a score on the leaderboard:\n",
    "- Try limiting the depth of the trees\n",
    "- Try using different voting methods (weighted voting, etc.)\n",
    "- Try using different methods to select the best split (information gain, gini index, etc.)\n",
    "\n",
    "Try these ideas **ONLY AFTER** you have a working implementation and a score on the leaderboard. These ideas are not guaranteed to improve your score but are worth trying out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Feature-1</th>\n",
       "      <th>Feature-2</th>\n",
       "      <th>Feature-3</th>\n",
       "      <th>Feature-4</th>\n",
       "      <th>Feature-5</th>\n",
       "      <th>Feature-6</th>\n",
       "      <th>Feature-7</th>\n",
       "      <th>Feature-8</th>\n",
       "      <th>Feature-9</th>\n",
       "      <th>Feature-10</th>\n",
       "      <th>Feature-11</th>\n",
       "      <th>Feature-12</th>\n",
       "      <th>Feature-13</th>\n",
       "      <th>Feature-14</th>\n",
       "      <th>Feature-15</th>\n",
       "      <th>Feature-16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Feature-1  Feature-2  Feature-3  Feature-4  Feature-5  Feature-6  \\\n",
       "0   1          0          1          0          1          1          1   \n",
       "1   2          0          1          0          1          1          1   \n",
       "2   3          1          1          0          1          1          1   \n",
       "3   4          1          1          1          0          0          0   \n",
       "4   5          1          1          1          0          0          0   \n",
       "\n",
       "   Feature-7  Feature-8  Feature-9  Feature-10  Feature-11  Feature-12  \\\n",
       "0          0          0          0           1           0           1   \n",
       "1          0          0          0           0           0           1   \n",
       "2          0          0          0           0           1           1   \n",
       "3          1          1          1           0           1           0   \n",
       "4          1          1          0           1           0           0   \n",
       "\n",
       "   Feature-13  Feature-14  Feature-15  Feature-16  \n",
       "0           1           1           0           0  \n",
       "1           1           1           0           1  \n",
       "2           1           1           0           1  \n",
       "3           0           0           0           1  \n",
       "4           0           0           0           1  "
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv')\n",
    "# encode the categorical columns\n",
    "d = {'y': 1, 'n': 0}\n",
    "for col in df_test.columns:\n",
    "    if col != 'ID':\n",
    "        df_test[col] = df_test[col].map(d)\n",
    "        \n",
    "    \n",
    "        \n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Party\n",
       "0   1      1\n",
       "1   2      1\n",
       "2   3      1\n",
       "3   4      0\n",
       "4   5      0"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict using bootstrap aggregation\n",
    "n_trees = 10\n",
    "sample_fraction = 0.8\n",
    "target = \"Party\" \n",
    "trees = []\n",
    "\n",
    "bootstrap_samples = get_bootstrap_samples(df_train, n_trees, sample_fraction, target)\n",
    "\n",
    "# Write your code below\n",
    "\n",
    "# Create a decision tree for each bootstrap sample and append it to the trees list\n",
    "for _s in bootstrap_samples:\n",
    "    tree = DecisionTree()\n",
    "    tree.fit(_s, features, target)\n",
    "    trees.append(tree)\n",
    "\n",
    "# Write code to predict the class of the current set of features and append the prediction to the preds list\n",
    "# Also append the decisions made by the model to the decisions list\n",
    "preds = []\n",
    "decisions = []\n",
    "for _, row in df_test.iterrows():\n",
    "    decision_importance = []\n",
    "    for tree in trees:\n",
    "        pred, decision = tree.predict(row)\n",
    "        decision_importance.append(pred)\n",
    "        decisions.append(decision)\n",
    "    \n",
    "    pred = max(set(decision_importance), key=decision_importance.count)\n",
    "    preds.append(pred)\n",
    "\n",
    "# add column to test data\n",
    "df_test['Party'] = preds\n",
    "\n",
    "# create df_submission with only ID and Party columns\n",
    "df_submission = df_test[['ID', 'Party']]\n",
    "df_submission.to_csv('submission.csv', index=False)\n",
    "df_submission.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick your ensemble model and use it to make predictions on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Cells\n",
    "\n",
    "We will now zip and prepare the notebook and csv for submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary checks to ensure `submission.csv` is in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_csv('submission.csv')\n",
    "test_temp = pd.read_csv('test.csv')\n",
    "assert len(df_temp.columns) == 2, \"Number of columns in the submission file is not correct, check the submission format\"\n",
    "assert list(df_temp.columns) == ['ID', 'Party'] , \"Column names are not correct, check the submission format\"\n",
    "assert df_temp['Party'].nunique() == 1 or df_temp['Party'].nunique() == 2, \"The prediction should be 0 or 1 only\"\n",
    "assert len(df_temp) == len(test_temp), \"Number of rows in the submission file is not correct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the submission zip ready<br>\n",
    "Note: Ensure that your notebook has been saved uptil now with the name eval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "if not os.path.exists('temp'):\n",
    "    os.makedirs('temp')\n",
    "\n",
    "if os.path.exists('submission.csv'):\n",
    "    shutil.copy('submission.csv','temp/submission.csv')\n",
    "\n",
    "if os.path.exists('eval.ipynb'):\n",
    "    shutil.copy('eval.ipynb',os.path.join('temp','eval.ipynb'))\n",
    "\n",
    "shutil.make_archive('submission', 'zip', 'temp')\n",
    "shutil.rmtree('temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the `submission.zip` file to kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
