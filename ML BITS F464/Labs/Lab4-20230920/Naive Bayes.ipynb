{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes and Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "from math import log\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will be trying classification models, namely:\n",
    "- Naive Bayes Classification\n",
    "- Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a classification technique based on Bayes' Theorem. We use Bayes' Theorem to find the probability of the target variable given the features.\n",
    "\n",
    "$$ P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)} $$\n",
    "\n",
    "where,\n",
    "- $Y$ is the target variable\n",
    "- $X$ is the feature variable\n",
    "- $P(Y|X)$ is the posterior probability of the target given features\n",
    "- $P(X|Y)$ is the likelihood which is the probability of features given the target\n",
    "- $P(Y)$ is the prior probability of the target\n",
    "- $P(X)$ is the prior probability of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find the `y` that maximizes the posterior probability $P(Y|X)$.  \n",
    "Notice that we can ignore the denominator $P(X)$ since it is constant for all classes.  \n",
    "\n",
    "We can then formulate our classifier as:\n",
    "\n",
    "$$ \\hat{y} = \\underset{y}{\\operatorname{argmax}} P(Y=y|X) = \\underset{y}{\\operatorname{argmax}} P(X|Y=y)P(Y=y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the training dataset into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('car_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 6 different features and a `Decision` target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So every single feature here is a categorical feature. Let's try to see which all are the unique values in each of these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.copy().apply(lambda x: x.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll convert this to a dictionary for later usage\n",
    "unique_values = df.copy().apply(lambda x: x.unique()).to_dict()\n",
    "unique_values = {k: v.tolist() for k, v in unique_values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following is the form of the dictionary\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Prior and Likelihood values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that for Naive Bayes, we essentially want to calculate\n",
    "\n",
    "$$ P(Y=y|X) \\propto P(X|Y=y)P(Y=y) $$\n",
    "\n",
    "where $X$ is the feature vector and $Y$ is the target variable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prior Probabilities $P(Y=y)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here You will calculate the prior probabilties of each class and store them as a python list:\n",
    "\n",
    "$$\n",
    "    prior\\_values = \\begin{bmatrix}\n",
    "        p_1 & p_2 & p_3 & p_4\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "  \n",
    "where,\n",
    "- $p_1 = P(Decision = unacc)$\n",
    "- $p_2 = P(Decision = acc)$\n",
    "- $p_3 = P(Decision = good)$\n",
    "- $p_4 = P(Decision = vgood)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can do this in a single line\n",
    "prior_list = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert this prior list to a pandas dataframe and display it\n",
    "prior_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Likelihood Probabilities $P(X|Y)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main assumption in Naive Bayes is that the features are conditionally independent given the target variable.  \n",
    "This means that we can calculate the likelihood probabilities as:\n",
    "\n",
    "$$ P(X|Y=y) = \\prod_{i=1}^{n} P(X_i|Y=y) $$\n",
    "\n",
    "where $X_i$ is the $i^{th}$ feature of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example of a single feature (say buying price) and calculate the likelihood probabilities for each class.  \n",
    "What we want is a pandas dataframe that looks something like this:\n",
    "\n",
    "| - | unacc | acc | good | vgood |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| vhigh | - | - | - | - |\n",
    "| high | - | - | - | - |\n",
    "| med | - | - | - | - |\n",
    "| low | - | - | - | - |\n",
    "\n",
    "where each cell value is the likelihood probability of that feature value given the class. You will need to do this for each and every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_feature_1 = None\n",
    "likelihood_feature_2 = None\n",
    "likelihood_feature_3 = None\n",
    "likelihood_feature_4 = None\n",
    "likelihood_feature_5 = None\n",
    "likelihood_feature_6 = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Calculation of Posterior Probabilities $P(Y|X)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall,\n",
    "\n",
    "we want to find the class $\\hat{y}$ that maximizes the posterior probability $P(Y|X)$ according to the formula:\n",
    "\n",
    "$$ \\hat{y} = \\underset{y}{\\operatorname{argmax}} P(Y=y|X) = \\underset{y}{\\operatorname{argmax}} P(X|Y=y)P(Y=y) $$\n",
    "\n",
    "Since, we assume that all the features are conditionally independent (Naive Bayes Assumption), we can rewrite our objective as:\n",
    "\n",
    "$$ \\hat{y} = \\underset{y}{\\operatorname{argmax}} \\prod_{i=1}^{n} P(X_i|Y=y) P(Y=y) $$\n",
    "\n",
    "\n",
    "For numerical stability, we will instead maximise the Log Posterior instead of just the Posterior, with this we can rewrite this as:\n",
    "\n",
    "$$ \\hat{y} = \\underset{y}{\\operatorname{argmax}} ( \\log P(Y=y) + \\sum_{i=1}^{n} \\log P(X_i|Y=y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a single example.\n",
    "\n",
    "Assume that we have a single example with the following feature values:\n",
    "\n",
    "| buying | maint | doors | capacity | lug_boot | safety |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| low | vhigh | 2 | more | med | high |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the posterior probabilities for each class for this example.  \n",
    "  \n",
    "Do the following steps for each class:\n",
    "- Calculate the log-likelihood probability for each feature value and add them together\n",
    "- Add the above value with the log-prior probability of that class\n",
    "- Store the result in a pandas dataframe\n",
    "\n",
    "You should get a pandas dataframe that looks something like this:\n",
    "\n",
    "| - | unacc | acc | good | vgood |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Log-Posterior | - | - | - | - |\n",
    "\n",
    "where each cell value is the posterior probability of that feature value given the class. You will need to do this for each and every feature.\n",
    "\n",
    "NOTE: Use the likelihood/prior values that you calculated earlier.\n",
    "\n",
    "I'll do it for the category `unacc`. Implement the rest in a similar fashion. Also feel free to change the code if you want to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's store the log prior\n",
    "\n",
    "features = ['low', 'vhigh', '2', 'more', 'med', 'high']\n",
    "\n",
    "log_prior = prior_df.loc['unacc', 'Prior']\n",
    "\n",
    "log_likelihood = 0\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    log_likelihood += log(eval(f'likelihood_feature_{i+1}').loc['unacc', feature] + 1e-9) # Add the 1e-9 to avoid taking log(0)\n",
    "\n",
    "log_posterior = log_prior + log_likelihood\n",
    "\n",
    "print(f'The Log Posterior for class `unacc` is: {log_posterior:.3f}')\n",
    "\n",
    "\n",
    "#! Hint: Convert this to a function which takes the class_name and features as parameters and return the log_posterior\n",
    "#! This should help you in the next section as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_posterior(class_name: str, features: list):\n",
    "    \"\"\"Returns the log posterior for the class name given the features\n",
    "\n",
    "    Args:\n",
    "        class_name (str): class name. [unacc, acc, good, vgood]\n",
    "        features (list): list of features\n",
    "    \"\"\"\n",
    "\n",
    "    # Write you code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the log-posterior, predict what the decision is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction with Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to do this for a single datapoint, let's do this for the whole test set.\n",
    "\n",
    "Let's start with same old loading of the test set into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('car_test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataframe into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df_test.drop('Decision', axis=1)\n",
    "df_y = df_test['Decision']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the features in `df_x`, predict the `Decision` using in the same way as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for _, row in df_x.iterrows():\n",
    "    features = list(row)\n",
    "    \n",
    "    # Write you code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the values of the prediction with the actual values. What is the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_values = list(df_y)\n",
    "\n",
    "correct = 0\n",
    "for pred, true_val in zip(predictions, true_values):\n",
    "    if pred == true_val:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(true_values)\n",
    "\n",
    "print(f'Test Accuracy using Naive Bayes Classifier is: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
