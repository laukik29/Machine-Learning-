{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Decision Tree Algorithm` is a non-parametric supervised learning method used for classification and regression tasks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.  \n",
    "  \n",
    "There are many different algorithms used to implement decision trees. In this notebook, we will implement the most basic one, which is called [ID3](https://en.wikipedia.org/wiki/ID3_algorithm). It is a greedy algorithm that builds a decision tree by selecting the best attribute that yields the highest information gain for each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with a few definitions:\n",
    "\n",
    "- **Entropy**: Entropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information. In other words, higher entropy means higher uncertainty. Entropy is defined as:\n",
    "\n",
    "$$H(X) = \\mathbb{E} [- \\log p(X)] =  -\\sum_{x \\in X} p(x) \\log p(x)$$\n",
    "\n",
    "where $p(x_i)$ is the probability of the $i^{th}$ outcome. For example, if we have a coin, the probability of getting heads is $p(x_i) = 0.5$ and the probability of getting tails is $p(x_i) = 0.5$. Therefore, the entropy of the coin is:\n",
    "\n",
    "$$H(X) = -0.5 \\log 0.5 - 0.5 \\log 0.5 = 1$$\n",
    "\n",
    "\n",
    "- **Information Gain**: Information gain is the measure of the difference in entropy from before to after the set $S$ is split on an attribute $A$. In other words, how much uncertainty in $S$ was reduced after splitting set $S$ on attribute $A$. Information gain is defined as:\n",
    "\n",
    "$$IG(S, A) = H(S) - \\sum_{t \\in T} \\frac{|t|}{|S|} H(t)$$\n",
    "\n",
    "where $S$ is the set of all samples at the current node, $A$ is an attribute being tested, $T$ is the set of all possible subsets of $S$ resulting from splitting on attribute $A$, and $H(t)$ is the entropy of subset $t$.\n",
    "\n",
    "\n",
    "The main essence of the ID3 algorithm is to select the attribute that has the highest information gain to split the data at each node. The algorithm will stop when all attributes have been used or when all instances of the node belong to the same class. (i.e. the entropy is zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's start with loading the training datasets into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Buying Price</th>\n",
       "      <th>Maintenance Price</th>\n",
       "      <th>Number of Doors</th>\n",
       "      <th>Capacity</th>\n",
       "      <th>Size of Luggage Boot</th>\n",
       "      <th>Estimated Safety</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low</td>\n",
       "      <td>low</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>low</td>\n",
       "      <td>high</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>big</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>low</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>med</td>\n",
       "      <td>5more</td>\n",
       "      <td>more</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>low</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>4</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>high</td>\n",
       "      <td>acc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Buying Price Maintenance Price Number of Doors Capacity  \\\n",
       "0          low               low               3        2   \n",
       "1          low              high               4        2   \n",
       "2        vhigh               low               3        4   \n",
       "3        vhigh               med           5more     more   \n",
       "4          low             vhigh               4     more   \n",
       "\n",
       "  Size of Luggage Boot Estimated Safety Decision  \n",
       "0                small             high    unacc  \n",
       "1                  big             high    unacc  \n",
       "2                small             high      acc  \n",
       "3                small              med    unacc  \n",
       "4                  med             high      acc  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('car_train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, dataset: pd.DataFrame, features: list, target: str):\n",
    "        \"\"\"Initializes the Node class. This class represents the current state of the dataset after a split has been made.\n",
    "\n",
    "        dataset : Current state of the dataset\n",
    "        features : A list of features to consider for the split\n",
    "        target : The target variable\n",
    "\n",
    "        best_feature: Feature which gives the highest information gain. This starts off as None and is updated after the split\n",
    "        best_feature_values: A dictionary which contains the unique values of the best feature and the corresponding subset of the dataset\n",
    "\n",
    "        class_names: A list of the unique values in the target variable\n",
    "        target_probabilities: A NumPy array which contains the probabilities of each unique value in the target attribute\n",
    "        entropy: Entropy of the target variable in the given dataset\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): Current state of the dataset\n",
    "            features (list): A list of features to consider for the split\n",
    "            target (str): The target variable\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "\n",
    "        self.best_feature = None\n",
    "        self.best_feature_values = {}\n",
    "\n",
    "        self.class_names = dataset[target].unique().tolist()\n",
    "        self.target_probabilities = self.calculate_target_probabilities(self.dataset)\n",
    "        self.entropy = self.calculate_entropy(self.dataset)\n",
    "    \n",
    "    def calculate_target_probabilities(self, dataset: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"This function calculates the probabilities of each unique value in the target variable.\n",
    "        You can think of this as the prior probabilities (as in Naive Bayes)\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): The dataset over which we want to calculate the probabilities\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Returns a NumPy array which contains the probabilities of each unique value in the target attribute\n",
    "        \"\"\"\n",
    "        target_probabilties = dataset[self.target].value_counts(normalize=True).values\n",
    "        return target_probabilties\n",
    "    \n",
    "    def calculate_entropy(self, dataset: pd.DataFrame) -> float:\n",
    "        \"\"\"Calculates the entropy of the target variable in the given dataset\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): dataset over which we want to calculate the entropy\n",
    "\n",
    "        Returns:\n",
    "            float: Entropy of the target variable in the given dataset\n",
    "        \"\"\"\n",
    "        target_probabilities = dataset[self.target].value_counts(normalize=True).values\n",
    "        target_log_probabilities = np.log2(target_probabilities)\n",
    "\n",
    "        entropy = -np.sum(target_probabilities * target_log_probabilities)\n",
    "        return entropy\n",
    "\n",
    "    def calculate_information_gain(self, feature: str) -> float:\n",
    "        \"\"\"Calculates the information gain for the gievn feature\n",
    "\n",
    "        Args:\n",
    "            feature (str): Feature for which we want to calculate the information gain\n",
    "\n",
    "        Returns:\n",
    "            float: Information gain for the given feature\n",
    "        \"\"\"\n",
    "\n",
    "        # Find the unique values of the feature\n",
    "        feature_values = self.dataset[feature].unique()\n",
    "\n",
    "        # Find the probabilities of each unique value\n",
    "        feature_probabilities = self.dataset[feature].value_counts(normalize=True).values\n",
    "\n",
    "        # Find the entropy of each unique value\n",
    "        feature_entropies = []\n",
    "        for feature_value in feature_values:\n",
    "            feature_subset = self.dataset[self.dataset[feature] == feature_value]\n",
    "            feature_subset_entropy = self.calculate_entropy(feature_subset)\n",
    "            feature_entropies.append(feature_subset_entropy)\n",
    "\n",
    "        # Find the weighted average of the entropies\n",
    "        feature_entropies = np.array(feature_entropies)\n",
    "        weighted_average = np.sum(feature_probabilities * feature_entropies)\n",
    "\n",
    "        # Find the information gain\n",
    "        information_gain = self.entropy - weighted_average\n",
    "        return information_gain\n",
    "\n",
    "    def find_best_feature(self) -> str:\n",
    "        \"\"\"Finds the best feature which gives the highest information gain. This is the feature which we will use to split the dataset\n",
    "\n",
    "        Returns:\n",
    "            str: Feature which gives the highest information gain\n",
    "        \"\"\"\n",
    "        # Get information gain for all features and select the one with the highest information gain\n",
    "        information_gains = []\n",
    "        for feature in self.features:\n",
    "            information_gain = self.calculate_information_gain(feature)\n",
    "            information_gains.append(information_gain)\n",
    "\n",
    "        information_gains = np.array(information_gains)\n",
    "        best_feature_index = np.argmax(information_gains)\n",
    "        best_feature = self.features[best_feature_index]\n",
    "        return best_feature\n",
    "    \n",
    "    def is_pure(self) -> bool:\n",
    "        \"\"\"Checks if the node is pure. A node is pure if all the target values are the same\n",
    "\n",
    "        Returns:\n",
    "            bool: Pure or not\n",
    "        \"\"\"\n",
    "        return self.entropy == 0\n",
    "\n",
    "    def predict(self) -> str:\n",
    "        \"\"\"Predicts the class of the current node. This is the class which has the highest probability\n",
    "\n",
    "        Returns:\n",
    "            str: Prediction\n",
    "        \"\"\"\n",
    "        idx = np.argmax(self.target_probabilities)\n",
    "        prediction = self.class_names[idx]\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def fit(self, dataset: pd.DataFrame, features: list, target: str, node: Node=None, depth: int=1):\n",
    "        \"\"\"Trains the decision tree model on the given dataset\n",
    "\n",
    "        Args:\n",
    "            dataset (pd.DataFrame): Dataset on which we want to train the model\n",
    "            features (list): List of features to consider for training\n",
    "            target (str): The target attribute\n",
    "            node (Node): The current node. Defaults to None.\n",
    "            depth (int): Current depth of the tree.\n",
    "        \"\"\"\n",
    "\n",
    "        if node is not None and node.is_pure(): # Do not split if the node is pure\n",
    "            return\n",
    "        \n",
    "        if self.max_depth is not None and depth > self.max_depth: # Do not split if the max depth is reached\n",
    "            return\n",
    "        \n",
    "        if not features: # Do not split if there are no features left\n",
    "            return\n",
    "        \n",
    "        if node is None:\n",
    "            node = Node(dataset, features, target)\n",
    "            self.root = node\n",
    "        \n",
    "        # Find the best feature to split on\n",
    "        best_feature = node.find_best_feature()\n",
    "        node.best_feature = best_feature # Set the best feature of the node\n",
    "        best_feature_values = dataset[best_feature].unique() # Find the unique values of the best feature\n",
    "\n",
    "        for best_feature_value in best_feature_values:\n",
    "\n",
    "            # Create a subset of the dataset which contains only the current best feature value and remove the best feature from the dataset\n",
    "            #! NOTE: Remember to create a copy of the dataset while removing the feature. Otherwise, the original dataset will be modified\n",
    "            best_feature_subset = dataset[dataset[best_feature] == best_feature_value]\n",
    "            best_feature_subset = best_feature_subset.drop(columns=[best_feature])\n",
    "\n",
    "            # Create a subset of the features which does not contain the best feature\n",
    "            best_feature_subset_features = list(best_feature_subset.columns)\n",
    "            best_feature_subset_features.remove(target)\n",
    "\n",
    "            # Create a new node for the best split\n",
    "            best_feature_subset_root = Node(best_feature_subset, best_feature_subset_features, target)\n",
    "            node.best_feature_values[best_feature_value] = best_feature_subset_root\n",
    "\n",
    "\n",
    "            # Recursively fit the model on the best feature subset\n",
    "            self.fit(best_feature_subset, best_feature_subset_features, target, node=best_feature_subset_root, depth=depth+1)\n",
    "        \n",
    "    def predict(self, features: pd.Series) -> tuple:\n",
    "        \"\"\"Predict the class for the given features using the trained model\n",
    "\n",
    "        Args:\n",
    "            features (pd.Series): features\n",
    "\n",
    "        Returns:\n",
    "            str: Prediction of the class\n",
    "            list: List of decisions made by the model. These are the features on which the model split the dataset\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        node = self.root\n",
    "        decisions = []\n",
    "\n",
    "        while node.best_feature is not None:\n",
    "            decisions.append(node.best_feature)\n",
    "            feature_value = features[node.best_feature]\n",
    "            if feature_value not in node.best_feature_values:\n",
    "                break\n",
    "            node = node.best_feature_values[feature_value]\n",
    "        \n",
    "        prediction = node.predict()\n",
    "        return prediction, decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get our list of features for our decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.tolist()\n",
    "features.remove('Decision')\n",
    "target = 'Decision'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the tree to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(df, features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Buying Price</th>\n",
       "      <th>Maintenance Price</th>\n",
       "      <th>Number of Doors</th>\n",
       "      <th>Capacity</th>\n",
       "      <th>Size of Luggage Boot</th>\n",
       "      <th>Estimated Safety</th>\n",
       "      <th>Decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>low</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>more</td>\n",
       "      <td>med</td>\n",
       "      <td>high</td>\n",
       "      <td>acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>high</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>big</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high</td>\n",
       "      <td>med</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>med</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>big</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>low</td>\n",
       "      <td>med</td>\n",
       "      <td>5more</td>\n",
       "      <td>2</td>\n",
       "      <td>big</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Buying Price Maintenance Price Number of Doors Capacity  \\\n",
       "0          low             vhigh               2     more   \n",
       "1        vhigh              high               2        4   \n",
       "2         high               med               2        2   \n",
       "3        vhigh               med               3        2   \n",
       "4          low               med           5more        2   \n",
       "\n",
       "  Size of Luggage Boot Estimated Safety Decision  \n",
       "0                  med             high      acc  \n",
       "1                  big             high    unacc  \n",
       "2                small              med    unacc  \n",
       "3                  big              med    unacc  \n",
       "4                  big              low    unacc  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('car_test.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "decisions = []\n",
    "for _, row in df_test.iterrows():\n",
    "    # Write code to predict the class of the current set of features and append the prediction to the preds list\n",
    "    # Also append the decisions made by the model to the decisions list\n",
    "    \n",
    "    pred, decision = clf.predict(row)\n",
    "    preds.append(pred)\n",
    "    decisions.append(decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9386503067484663\n"
     ]
    }
   ],
   "source": [
    "# Let's check the accuracy of the model\n",
    "\n",
    "true = df_test['Decision'].tolist()\n",
    "\n",
    "correct = 0\n",
    "for i in range(len(true)):\n",
    "    if true[i] == preds[i]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(true)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
