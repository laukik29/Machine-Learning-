{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exploring the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with loading the training data from the csv into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_processed_splitted.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the first 5 rows of this dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11553</td>\n",
       "      <td>1051</td>\n",
       "      <td>1159</td>\n",
       "      <td>336</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8400</td>\n",
       "      <td>1052</td>\n",
       "      <td>1052</td>\n",
       "      <td>288</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>138500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8960</td>\n",
       "      <td>1008</td>\n",
       "      <td>1028</td>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11100</td>\n",
       "      <td>0</td>\n",
       "      <td>930</td>\n",
       "      <td>308</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>84900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15593</td>\n",
       "      <td>1304</td>\n",
       "      <td>2287</td>\n",
       "      <td>667</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>225000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LotArea  TotalBsmtSF  GrLivArea  GarageArea  PoolArea  OverallCond  \\\n",
       "0    11553         1051       1159         336         0            5   \n",
       "1     8400         1052       1052         288         0            5   \n",
       "2     8960         1008       1028         360         0            6   \n",
       "3    11100            0        930         308         0            7   \n",
       "4    15593         1304       2287         667         0            4   \n",
       "\n",
       "  Utilities  SalePrice  \n",
       "0    AllPub     158000  \n",
       "1    AllPub     138500  \n",
       "2    AllPub     115000  \n",
       "3    AllPub      84900  \n",
       "4    AllPub     225000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are all the features present? What is the range for each of the features along with their mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1314.000000</td>\n",
       "      <td>1314.000000</td>\n",
       "      <td>1314.000000</td>\n",
       "      <td>1314.000000</td>\n",
       "      <td>1314.000000</td>\n",
       "      <td>1314.000000</td>\n",
       "      <td>1314.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10622.104262</td>\n",
       "      <td>1058.311263</td>\n",
       "      <td>1512.900304</td>\n",
       "      <td>473.480213</td>\n",
       "      <td>2.643075</td>\n",
       "      <td>5.582192</td>\n",
       "      <td>180795.504566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10430.181058</td>\n",
       "      <td>435.717809</td>\n",
       "      <td>524.854432</td>\n",
       "      <td>213.960987</td>\n",
       "      <td>39.504255</td>\n",
       "      <td>1.112699</td>\n",
       "      <td>77511.272784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1300.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>334.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>34900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7588.500000</td>\n",
       "      <td>796.000000</td>\n",
       "      <td>1124.250000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>130000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9501.500000</td>\n",
       "      <td>992.000000</td>\n",
       "      <td>1461.500000</td>\n",
       "      <td>480.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>163250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11613.500000</td>\n",
       "      <td>1295.250000</td>\n",
       "      <td>1775.750000</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>215000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>215245.000000</td>\n",
       "      <td>6110.000000</td>\n",
       "      <td>5642.000000</td>\n",
       "      <td>1418.000000</td>\n",
       "      <td>738.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>755000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             LotArea  TotalBsmtSF    GrLivArea   GarageArea     PoolArea  \\\n",
       "count    1314.000000  1314.000000  1314.000000  1314.000000  1314.000000   \n",
       "mean    10622.104262  1058.311263  1512.900304   473.480213     2.643075   \n",
       "std     10430.181058   435.717809   524.854432   213.960987    39.504255   \n",
       "min      1300.000000     0.000000   334.000000     0.000000     0.000000   \n",
       "25%      7588.500000   796.000000  1124.250000   336.000000     0.000000   \n",
       "50%      9501.500000   992.000000  1461.500000   480.000000     0.000000   \n",
       "75%     11613.500000  1295.250000  1775.750000   576.000000     0.000000   \n",
       "max    215245.000000  6110.000000  5642.000000  1418.000000   738.000000   \n",
       "\n",
       "       OverallCond      SalePrice  \n",
       "count  1314.000000    1314.000000  \n",
       "mean      5.582192  180795.504566  \n",
       "std       1.112699   77511.272784  \n",
       "min       1.000000   34900.000000  \n",
       "25%       5.000000  130000.000000  \n",
       "50%       5.000000  163250.000000  \n",
       "75%       6.000000  215000.000000  \n",
       "max       9.000000  755000.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Scaling and One-Hot Encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must have noticed that some features `(such as Utilities)` are not continuous values.\n",
    "  \n",
    "These features contain values indicating different categories and must somehow be converted to numbers so that the computer can understand it. `(Computers only understand numbers and not strings)`\n",
    "  \n",
    "These features are called categorical features. We can represent these features as a `One-Hot Representation`\n",
    "  \n",
    "  \n",
    "You must have also noticed that all the other features, each are in a different scale. This can be detremental to the performance of our linear regression model and so we normalize them so that all of them are in the range $[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NOTE: When you are doing feature scaling, store the min/max which you will use to normalize somewhere. This is then to be used at testing time. Try to think why are doing this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11553.0</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>1159.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>158000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8400.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>138500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8960.0</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>1028.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>115000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>930.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84900.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15593.0</td>\n",
       "      <td>1304.0</td>\n",
       "      <td>2287.0</td>\n",
       "      <td>667.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LotArea  TotalBsmtSF  GrLivArea  GarageArea  PoolArea  OverallCond  \\\n",
       "0  11553.0       1051.0     1159.0       336.0       0.0          5.0   \n",
       "1   8400.0       1052.0     1052.0       288.0       0.0          5.0   \n",
       "2   8960.0       1008.0     1028.0       360.0       0.0          6.0   \n",
       "3  11100.0          0.0      930.0       308.0       0.0          7.0   \n",
       "4  15593.0       1304.0     2287.0       667.0       0.0          4.0   \n",
       "\n",
       "   SalePrice  Utilities_AllPub  Utilities_NoSeWa  \n",
       "0   158000.0               1.0               0.0  \n",
       "1   138500.0               1.0               0.0  \n",
       "2   115000.0               1.0               0.0  \n",
       "3    84900.0               1.0               0.0  \n",
       "4   225000.0               1.0               0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the one-hot encoding here\n",
    "df = pd.get_dummies(df, columns=['Utilities']).astype(float)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047924</td>\n",
       "      <td>0.172013</td>\n",
       "      <td>0.155426</td>\n",
       "      <td>0.236953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.170948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.033186</td>\n",
       "      <td>0.172177</td>\n",
       "      <td>0.135268</td>\n",
       "      <td>0.203103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.143869</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.035804</td>\n",
       "      <td>0.164975</td>\n",
       "      <td>0.130746</td>\n",
       "      <td>0.253879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.111235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.045806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112283</td>\n",
       "      <td>0.217207</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.069435</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.066807</td>\n",
       "      <td>0.213421</td>\n",
       "      <td>0.367935</td>\n",
       "      <td>0.470381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.263991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    LotArea  TotalBsmtSF  GrLivArea  GarageArea  PoolArea  OverallCond  \\\n",
       "0  0.047924     0.172013   0.155426    0.236953       0.0        0.500   \n",
       "1  0.033186     0.172177   0.135268    0.203103       0.0        0.500   \n",
       "2  0.035804     0.164975   0.130746    0.253879       0.0        0.625   \n",
       "3  0.045806     0.000000   0.112283    0.217207       0.0        0.750   \n",
       "4  0.066807     0.213421   0.367935    0.470381       0.0        0.375   \n",
       "\n",
       "   SalePrice  Utilities_AllPub  Utilities_NoSeWa  \n",
       "0   0.170948               1.0               0.0  \n",
       "1   0.143869               1.0               0.0  \n",
       "2   0.111235               1.0               0.0  \n",
       "3   0.069435               1.0               0.0  \n",
       "4   0.263991               1.0               0.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do the feature scaling here\n",
    "\n",
    "# The output here is 'SalePrice'. Since the scale of the output is too large, we will scale is down to [0, 1].\n",
    "# When we train our linear regression model, it will learn to predict outputs in this same range ([0, 1]).\n",
    "# But what we need is the actual prediction of the SalePrice. Which is why we will scale our prediction back up to the original scale\n",
    "\n",
    "\n",
    "# Store the min/max values to be used at test time\n",
    "df_min = df.min()\n",
    "df_max = df.max()\n",
    "\n",
    "# Scale the features\n",
    "df = (df - df_min) / (df_max - df_min)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conversion to NumPy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so now that we have all preprocessed all the data, we need to convert it to numpy for our linear regression model\n",
    "  \n",
    "Assume that our dataset has a total of $N$ datapoints. Each datapoint having a total of $D$ features (after one-hot encoding), we want our numpy array to be of shape $(N, D)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our task, we have to predict the `SalePrice`. We will need 2 numpy arrays $(X, Y)$. These represent the features and targets respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy array\n",
    "\n",
    "x = df.copy().drop('SalePrice', axis=1).to_numpy() # (N, D)\n",
    "y = df.copy()['SalePrice'].to_numpy().reshape(-1, 1) # (N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression formulation\n",
    "  \n",
    "We now have our data in the form we need. Let's try to create a linear model to get our initial (Really bad) prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say a single datapoint in our dataset consists of 3 features $(x_1, x_2, x_3)$, we can pose it as a linear equation as follows:\n",
    "$$ y = w_1x_1 + w_2x_2 + w_3x_3 + b $$\n",
    "Here we have to learn 4 parameters $(w_1, w_2, w_3, b)$\n",
    "  \n",
    "  \n",
    "Now how do we extend this to multiple datapoints?  \n",
    "  \n",
    "  \n",
    "Try to answer the following:\n",
    "- How many parameters will we have to learn in the case of our dataset? (Don't forget the bias term)\n",
    "- Form a linear equation for our dataset. We need just a single matrix equation which correctly represents all the datapoints in our dataset\n",
    "- Implement the linear equation as an equation using NumPy arrays (Start by randomly initializing the weights from a standard normal distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will have to learn a total of **9** parameters in the case of our dataset.  \n",
    "We have **8** features (After One-Hot encoding, `Utitiles` is counted as 2 features. `SalePrice` is not a feature but a target) and **1** bias term  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For linear formulation, if we have `N` datapoints, each having `D` features, we have a total of `N` linear equations.\n",
    "\n",
    "$$y_1 = w_1x_1^{(1)} + w_2x_{(2)}^1 \\ldots w_Dx_D^{(1)} + b$$\n",
    "$$y_2 = w_1x_1^{(2)} + w_2x_2^{(2)} \\ldots w_Dx_D^{(2)} + b$$\n",
    "$$\\vdots$$\n",
    "$$y_N = w_1x_1^{(N)} + w_2x_2^{(N)} \\ldots w_Dx_D^{(N)} + b$$\n",
    "\n",
    "We can represent this in matrix form as follows:\n",
    "\n",
    "$$\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N\\end{bmatrix} = \\begin{bmatrix}x_1^{(1)} & x_2^{(1)} & \\ldots & x_D^{(1)} \\\\ x_1^{(2)} & x_2^{(2)} & \\ldots & x_D^{(2)} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_1^{(N)} & x_2^{(N)} & \\ldots & x_D^{(N)} \\end{bmatrix} \\begin{bmatrix}w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_D \\end{bmatrix} + \\begin{bmatrix} b \\\\ b \\\\ \\vdots \\\\ b \\end{bmatrix}$$\n",
    "\n",
    "So, if we can represent our dataset(only features) as a matrix $X$ of shape $(N, D)$, our targets as a matrix $Y$ of shape $(N, 1)$ and our weights as a matrix $W$ of shape $(1, D)$, we have our linear equation as: \n",
    "\n",
    "$$Y = XW^T + B$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implementation of our above formulation using random weights (and bias) in NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set a seed so that we can reproduce our results\n",
    "np.random.seed(42)\n",
    "\n",
    "w = np.random.randn(1, x.shape[1]) # (1, 8)\n",
    "b = np.random.randn(1, 1) # (1, 1)\n",
    "\n",
    "# x.shape = (1460, 8)\n",
    "# y.shape = (1460, 1)\n",
    "\n",
    "y_pred = x @ w.T + b # (1460, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our model perform? Try comparing our predictions with the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted SalePrice: \t [1193685, 1074239, 1328713, 1209386, 1158724]\n",
      "Actual SalePrice: \t [159950, 177000, 240000, 129500, 179540]\n"
     ]
    }
   ],
   "source": [
    "# Recall that during our preprocessing step, we scaled the outputs to be in the range [0, 1].\n",
    "# We need to scale our predictions back to the original scale so that we can compare it with the actual SalePrice\n",
    "\n",
    "y_pred = y_pred * (df_max['SalePrice'] - df_min['SalePrice']) + df_min['SalePrice']\n",
    "y_true = y * (df_max['SalePrice'] - df_min['SalePrice']) + df_min['SalePrice']\n",
    "\n",
    "# For the sake of simplicity, we will take 5 random samples from the dataset, round them to the nearest integer and compare their values\n",
    "\n",
    "idx = np.random.randint(0, x.shape[0], 5)\n",
    "y_pred_sample = y_pred[idx].round().astype(int)\n",
    "y_true_sample = y_true[idx].round().astype(int)\n",
    "\n",
    "print('Predicted SalePrice: \\t', y_pred_sample.squeeze().tolist())\n",
    "print('Actual SalePrice: \\t', y_true_sample.squeeze().tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our model performs really horribly. The values are way off. We need to improve our model. We will do this in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Learning weights using gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these results are really horrible. We need to somehow update our weights so that it correclty represents our data. How do we do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must do the following:\n",
    "- We need some numerical indication for our performance, for this we define a Loss Function ( $\\mathscr{L}$ )\n",
    "- Find the gradients of the `Loss` with respect to the `Weights`\n",
    "- Update the weights in accordance to the gradients: $W = W - \\alpha\\nabla_W \\mathscr{L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the loss function:\n",
    "- We will use the MSE loss since it is a regression task. (Specify the assumptions we make while doing so as taught in the class).\n",
    "- Implement this loss as a function. (Use numpy as much as possible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MSE Loss` is defined as $\\mathscr{L} = \\frac{1}{2N} \\sum_{i=1}^{N} (\\hat{y}^{(i)} - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_fn(y_true, y_pred):\n",
    "    return 0.5 * np.mean((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the gradients of the loss with respect to the weights (and biases). First write the equations down on a piece of paper, then proceed to implement it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients of the Loss function with respect to the weights (and biases) are given by:\n",
    "\n",
    "$$\\nabla_W \\mathscr{L} = \\frac{1}{N} (XW^T + B - Y)^TX$$\n",
    "$$\\nabla_B \\mathscr{L} = \\frac{1}{N} \\sum_{i=1}^{N} (XW^T + B - Y)$$  \n",
    "\n",
    "This can also be written as:\n",
    "\n",
    "$$\\nabla_W \\mathscr{L} = \\frac{1}{N} (\\hat{Y} - Y)^TX$$\n",
    "$$\\nabla_B \\mathscr{L} = \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{Y} - Y)$$  \n",
    "\n",
    "where $\\hat{Y} = XW^T + B$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we get this? Notice how the equations are matrices. We use something called `Jacobians` to calculate the gradients. You can read more about it in the resources section:\n",
    "- [Matrix Calculus](https://explained.ai/matrix-calculus/index.html)\n",
    "- [Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)\n",
    "\n",
    "\n",
    "If you don't want to use `Jacobians`,\n",
    "- Calculate the gradients of the loss function with respect to each weight (and bias) individually. You have done this in class\n",
    "- You will get a multiple equations. Combine them in the form of matrices\n",
    "- Make sure that the dimension of the gradients is the same as that of the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients(y_true, y_pred, W, b, X):\n",
    "    \"\"\"\n",
    "    Calculates the gradients for the MSE loss function with respect to the weights (and bias)\n",
    "\n",
    "    Args:\n",
    "        y_true: The true values of the target variable (SalePrice in our case)\n",
    "        y_pred: The predicted values of the target variable using our model (W*X + b)\n",
    "\n",
    "        W: The weights of the model\n",
    "        b: The bias of the model\n",
    "        X: The input features\n",
    "\n",
    "    Returns:\n",
    "        dW: The gradients of the loss function with respect to the weights\n",
    "        db: The gradients of the loss function with respect to the bias\n",
    "    \"\"\"\n",
    "    \n",
    "    dw = (y_pred - y_true).T @ X / X.shape[0]\n",
    "    db = np.mean(y_pred - y_true)\n",
    "\n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the weights using the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(weights, bias, gradients_weights, gradients_bias, lr):\n",
    "    \"\"\"\n",
    "    Updates the weights (and bias) using the gradients and the learning rate\n",
    "\n",
    "    Args:\n",
    "        weights: The current weights of the model\n",
    "        bias: The current bias of the model\n",
    "\n",
    "        gradients_weights: The gradients of the loss function with respect to the weights\n",
    "        gradients_bias: The gradients of the loss function with respect to the bias\n",
    "\n",
    "        lr: The learning rate\n",
    "\n",
    "    Returns:\n",
    "        weights_new: The updated weights of the model\n",
    "        bias_new: The updated bias of the model\n",
    "    \"\"\"\n",
    "\n",
    "    weights_new = weights - lr * gradients_weights\n",
    "    bias_new = bias - lr * gradients_bias\n",
    "\n",
    "    return weights_new, bias_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all these together to find the loss value, its gradient and finally updating the weights in a loop. Feel free to play around with different learning rates and epochs\n",
    "  \n",
    "> NOTE: The code in comments are just meant to be used as a guide. You will have to do changes based on your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1_000\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    y_pred = x @ w.T + b\n",
    "    loss = mse_loss_fn(y, y_pred)\n",
    "    losses.append(loss)\n",
    "    dw, db = get_gradients(y, y_pred, w, b, x)\n",
    "    w, b = update(w, b, dw, db, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x @ w.T + b\n",
    "y_pred_scaled = y_pred * (df_max['SalePrice'] - df_min['SalePrice']) + df_min['SalePrice']\n",
    "y_true_scaled = y * (df_max['SalePrice'] - df_min['SalePrice']) + df_min['SalePrice']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted SalePrice: \t [167831, 159075, 228728, 201759, 169889]\n",
      "Actual SalePrice: \t [159950, 177000, 240000, 129500, 179540]\n",
      "\n",
      "Final loss: \t\t 0.0029198333119779805\n"
     ]
    }
   ],
   "source": [
    "print('Predicted SalePrice: \\t', y_pred_scaled[idx].round().astype(int).squeeze().tolist())\n",
    "print('Actual SalePrice: \\t', y_true_scaled[idx].round().astype(int).squeeze().tolist())\n",
    "print('\\nFinal loss: \\t\\t', losses[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! Our model is able to learn specific patterns in the data. These patterns are captured by the weights (and biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use matplotlib to plot the loss graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing with test data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and apply all the preprocessing steps used in the training data for the testing data as well. Remember to use the **SAME** min/max values which you used for the training set and not recalculate them from the test set. Also mention why we are doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "- We are using the same min/max values for the test set as we used for the training set because we want to normalize the test set in the same way as we normalized the training set.\n",
    "- This is because we want to test our model on the same distribution as the training set.\n",
    "- If we use different min/max values, we will be testing our model on a different distribution than the training set and our model will not perform well on the test set\n",
    "- Let's also think about a scenario where we have just a single datapoint in our test set. What will happen if we calculate the min/max values from this single datapoint?\n",
    "- We will get a divide by zero error. This is because the min/max values will be the same and so the denominator will be zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotArea</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Utilities_AllPub</th>\n",
       "      <th>Utilities_NoSeWa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>146.0</td>\n",
       "      <td>146.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9569.342466</td>\n",
       "      <td>1049.493151</td>\n",
       "      <td>1538.534247</td>\n",
       "      <td>468.479452</td>\n",
       "      <td>3.801370</td>\n",
       "      <td>5.513699</td>\n",
       "      <td>182052.417808</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4041.729289</td>\n",
       "      <td>466.234413</td>\n",
       "      <td>532.345664</td>\n",
       "      <td>213.073412</td>\n",
       "      <td>45.932127</td>\n",
       "      <td>1.115634</td>\n",
       "      <td>95383.188980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1680.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>747.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7308.750000</td>\n",
       "      <td>793.250000</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>305.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>126250.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9245.500000</td>\n",
       "      <td>985.000000</td>\n",
       "      <td>1493.000000</td>\n",
       "      <td>471.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>161000.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11463.250000</td>\n",
       "      <td>1361.750000</td>\n",
       "      <td>1792.000000</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>202623.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35760.000000</td>\n",
       "      <td>3094.000000</td>\n",
       "      <td>4476.000000</td>\n",
       "      <td>1043.000000</td>\n",
       "      <td>555.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>745000.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            LotArea  TotalBsmtSF    GrLivArea   GarageArea    PoolArea  \\\n",
       "count    146.000000   146.000000   146.000000   146.000000  146.000000   \n",
       "mean    9569.342466  1049.493151  1538.534247   468.479452    3.801370   \n",
       "std     4041.729289   466.234413   532.345664   213.073412   45.932127   \n",
       "min     1680.000000     0.000000   747.000000     0.000000    0.000000   \n",
       "25%     7308.750000   793.250000  1201.000000   305.750000    0.000000   \n",
       "50%     9245.500000   985.000000  1493.000000   471.000000    0.000000   \n",
       "75%    11463.250000  1361.750000  1792.000000   576.000000    0.000000   \n",
       "max    35760.000000  3094.000000  4476.000000  1043.000000  555.000000   \n",
       "\n",
       "       OverallCond      SalePrice  Utilities_AllPub  Utilities_NoSeWa  \n",
       "count   146.000000     146.000000             146.0             146.0  \n",
       "mean      5.513699  182052.417808               1.0               0.0  \n",
       "std       1.115634   95383.188980               0.0               0.0  \n",
       "min       2.000000   60000.000000               1.0               0.0  \n",
       "25%       5.000000  126250.000000               1.0               0.0  \n",
       "50%       5.000000  161000.000000               1.0               0.0  \n",
       "75%       6.000000  202623.750000               1.0               0.0  \n",
       "max       9.000000  745000.000000               1.0               0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This test case is a special one. Notice that here Utilities_AllPub has 0 standard deviation.\n",
    "# This means that the value of this feature is the same for all the samples in the dataset.\n",
    "# This should not be an issue for our model at test time, but pd.get_dummies() will create only one column for this feature.\n",
    "# This will cause an issue at test time since the number of features in the test set will be different from the number of features in the training set.\n",
    "# To go around this issue, we will manually add a column (Utilities_NoSeWa) with all zeros for this feature in the test set.\n",
    "\n",
    "df_test = pd.read_csv('test_processed_splitted.csv')\n",
    "df_test = pd.get_dummies(df_test, columns=['Utilities']).astype(float)\n",
    "\n",
    "# Let's find all the columns that are missing in the test set\n",
    "missing_cols = set(df.columns) - set(df_test.columns)\n",
    "\n",
    "# Add these columns to the test set with all zeros\n",
    "for col in missing_cols:\n",
    "    df_test[col] = 0\n",
    "\n",
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's proceed with the rest of the preprocessing steps\n",
    "\n",
    "# Scale the features\n",
    "df_test = (df_test - df_min) / (df_max - df_min)\n",
    "\n",
    "# Convert to numpy array\n",
    "x_test = df_test.copy().drop('SalePrice', axis=1).to_numpy() # (N, D)\n",
    "y_test = df_test.copy()['SalePrice'].to_numpy().reshape(-1, 1) # (N, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the weights learnt above, predict the values in the test dataset. Also answer the following questions:\n",
    "- Are the predictions good?\n",
    "- What is the MSE loss for the testset\n",
    "- Is the MSE loss for testing greater or lower than training\n",
    "- Why is this the case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_test = x_test @ w.T + b # (N, 1)\n",
    "loss_test = mse_loss_fn(y_pred_test, y_test)\n",
    "\n",
    "# Scale the predictions back to the original scale\n",
    "y_pred_test_scaled = y_pred_test * (df_max['SalePrice'] - df_min['SalePrice']) + df_min['SalePrice']\n",
    "y_test_scaled = y_test * (df_max['SalePrice'] - df_min['SalePrice']) + df_min['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted SalePrice: \t [122276, 201379, 83920, 148472, 301182]\n",
      "Actual SalePrice: \t [121600, 196500, 85000, 170000, 410000]\n",
      "\n",
      "Test Loss: \t\t 0.005137542886225422\n"
     ]
    }
   ],
   "source": [
    "# Again for the sake of simplicity, let's take 5 random samples, round them to the nearest integer and compare their values\n",
    "\n",
    "idx = np.random.randint(0, x_test.shape[0], 5)\n",
    "y_pred_test_sample = y_pred_test_scaled[idx].round().astype(int)\n",
    "y_true_test_sample = y_test_scaled[idx].round().astype(int)\n",
    "\n",
    "print('Predicted SalePrice: \\t', y_pred_test_sample.squeeze().tolist())\n",
    "print('Actual SalePrice: \\t', y_true_test_sample.squeeze().tolist())\n",
    "print('\\nTest Loss: \\t\\t', loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**\n",
    "- The predictions are way better than those random predictions we made earlier, but they can be made better. Feel free to play around with some parameters such as learning rate and epochs to see what happens.  \n",
    "Also try the same without feature scaling and one-hot encoding to see what happens\n",
    "- The MSE loss for the test set is greater than the training set.  \n",
    "- - This is because the model has been trained specifically to get predictions **ONLY** for the training set. It has not seen the test set before and so it is not able to generalize as well for the test set as it did for the training set.\n",
    "  - Still, the performance is pretty good. What it has learnt for the training set is pretty much the same as what it has learnt for the test set.\n",
    "  - Notice that the distribution of the test set is pretty much the same as the training set. If the distribution of the test set was different, the performance would have been much worse.\n",
    "  - This condition for the model to generalize well is called `i.i.d` or `Independent and Identically Distributed`. You would have learnt about this in your Probability and Statistics course. For those who are not familiar with this, you can read more about it [here](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
